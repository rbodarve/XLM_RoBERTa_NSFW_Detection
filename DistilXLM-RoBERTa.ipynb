{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbc934dd",
   "metadata": {},
   "source": [
    "DistilXLM-RoBERTa for NSFW word detection and export to ONNX/TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0f0674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import tempfile\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bfae11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Ensure proper encoding on Windows\n",
    "if sys.platform.startswith('win'):\n",
    "    import locale\n",
    "    locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a8b732",
   "metadata": {},
   "source": [
    "IMPORT ADDITIONAL SETUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9808623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c7347b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(csv_path='dataset.csv'):\n",
    "    \"\"\"Load and validate the NSFW dataset\"\"\"\n",
    "    print(f\"Loading dataset from {csv_path}...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "        \n",
    "        # Validate required columns\n",
    "        required_cols = ['text', 'labels']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        # Clean and validate data\n",
    "        df = df.dropna(subset=['text', 'labels'])\n",
    "        df['text'] = df['text'].astype(str)\n",
    "        df['labels'] = df['labels'].astype(int)\n",
    "        \n",
    "        # Validate labels are binary (0 or 1)\n",
    "        unique_labels = df['labels'].unique()\n",
    "        if not all(labels in [0, 1] for labels in unique_labels):\n",
    "            raise ValueError(\"Labels must be 0 (safe) or 1 (nsfw)\")\n",
    "        \n",
    "        print(f\"Dataset validation complete. Clean shape: {df.shape}\")\n",
    "        print(f\"Labels distribution:\\n{df['labels'].value_counts()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Dataset file '{csv_path}' not found!\")\n",
    "        print(\"Creating a sample dataset for demonstration...\")\n",
    "        return create_sample_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "baed4afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_dataset():\n",
    "    \"\"\"Create a sample dataset for demonstration purposes\"\"\"\n",
    "    sample_data = {\n",
    "        'text': [\n",
    "            # Safe words/phrases\n",
    "            'hello', 'world', 'computer', 'programming', 'science', 'education',\n",
    "            'family', 'friendship', 'learning', 'knowledge', 'book', 'music',\n",
    "            'art', 'nature', 'technology', 'innovation', 'creativity', 'peace',\n",
    "            'happiness', 'success', 'achievement', 'progress', 'development',\n",
    "            'community', 'cooperation', 'collaboration', 'respect', 'kindness',\n",
    "            # NSFW words (examples - replace with actual dataset)\n",
    "            'yawa', 'gago', 'shit', 'pakyu',\n",
    "            'puta', 'pisti', 'puki', 'buang',\n",
    "            'motherfucker', 'bitch', 'cunt', 'ulol',\n",
    "            'fuck', 'gaga', 'tanga', 'dick',\n",
    "            'fucker', 'putangina', 'bobo', 'asshole'\n",
    "        ],\n",
    "        'labels': [0] * 28 + [1] * 20  # 28 safe, 20 nsfw\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(sample_data)\n",
    "    df.to_csv('dataset.csv', index=False)\n",
    "    print(\"Sample dataset created and saved as 'dataset.csv'\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "755cee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"Split dataset into training and validation sets\"\"\"\n",
    "    print(f\"Splitting dataset: {test_size*100}% for validation...\")\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        df['text'].tolist(),\n",
    "        df['labels'].tolist(),\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=df['labels']\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(X_train)} samples\")\n",
    "    print(f\"Validation set: {len(X_val)} samples\")\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de1aea3",
   "metadata": {},
   "source": [
    "TOKENIZATION AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5bd73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenized_datasets(X_train, X_val, y_train, y_val, model_name):\n",
    "    \"\"\"Tokenize the datasets using the model tokenizer\"\"\"\n",
    "    print(\"Loading tokenizer and creating tokenized datasets...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        'text': X_train,\n",
    "        'labels': y_train\n",
    "    })\n",
    "    \n",
    "    val_dataset = Dataset.from_dict({\n",
    "        'text': X_val,\n",
    "        'labels': y_val\n",
    "    })\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            padding=False,  # Will be handled by data collator\n",
    "            max_length=128  # Reasonable for word/short phrase detection\n",
    "        )\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_tokenized = val_dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    print(\"Tokenization complete!\")\n",
    "    return train_tokenized, val_tokenized, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12cf0f8",
   "metadata": {},
   "source": [
    "MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76d0213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataset, val_dataset, tokenizer, model_name, output_dir):\n",
    "    \"\"\"Train the DistilXLM-RoBERTa model\"\"\"\n",
    "    print(\"Initializing model for training...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,\n",
    "        id2label={0: \"SAFE\", 1: \"NSFW\"},\n",
    "        label2id={\"SAFE\": 0, \"NSFW\": 1}\n",
    "    )\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'{output_dir}/logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        report_to=[],  # Disable wandb/tensorboard\n",
    "        seed=42,\n",
    "        dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    "        remove_unused_columns=True\n",
    "    )\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        \"\"\"Compute metrics for evaluation\"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy_score(labels, predictions),\n",
    "        }\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the best model\n",
    "    print(f\"Saving model to {output_dir}...\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return trainer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce68ecd",
   "metadata": {},
   "source": [
    "MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88ed7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trainer, y_val, output_dir):\n",
    "    \"\"\"Evaluate the trained model and save metrics\"\"\"\n",
    "    print(\"Evaluating model performance...\")\n",
    "\n",
    "    # Get evaluation results\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    # Get detailed predictions for classification report\n",
    "    predictions = trainer.predict(trainer.eval_dataset)\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "    # Generate classification report\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    \n",
    "    report = classification_report(\n",
    "        y_val, y_pred, target_names=[\"SAFE\", \"NSFW\"], digits=4\n",
    "    )\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    # Prepare metrics text\n",
    "    metrics_text = f\"\"\"NSFW Detection Model Evaluation Results\n",
    "{'='*50}\n",
    "\n",
    "Accuracy: {accuracy:.4f}\n",
    "\n",
    "Classification Report:\n",
    "{report}\n",
    "\n",
    "Training Results:\n",
    "{'-'*30}\n",
    "\"\"\"\n",
    "\n",
    "    for key, value in eval_results.items():\n",
    "        metrics_text += f\"{key}: {value:.4f}\\n\"\n",
    "\n",
    "    # Save metrics\n",
    "    os.makedirs(\"metrics\", exist_ok=True)\n",
    "    metrics_path = \"metrics/metrics.txt\"\n",
    "\n",
    "    with open(metrics_path, \"w\", encoding='utf-8') as f:  # Add encoding for Windows\n",
    "        f.write(metrics_text)\n",
    "\n",
    "    print(f\"Metrics saved to {metrics_path}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a462e5e1",
   "metadata": {},
   "source": [
    "ONNX EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9d79f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_onnx(model_dir, onnx_path):\n",
    "    \"\"\"Export the trained model to ONNX format using Optimum\"\"\"\n",
    "    print(\"Exporting model to ONNX format...\")\n",
    "\n",
    "    try:\n",
    "        from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "        from optimum.exporters.onnx import main_export\n",
    "        from pathlib import Path\n",
    "        \n",
    "        # Create ONNX directory\n",
    "        onnx_dir = os.path.dirname(onnx_path)\n",
    "        os.makedirs(onnx_dir, exist_ok=True)\n",
    "\n",
    "        # Method 1: Use ORTModelForSequenceClassification\n",
    "        try:\n",
    "            onnx_model = ORTModelForSequenceClassification.from_pretrained(\n",
    "                model_dir, \n",
    "                export=True,\n",
    "                use_cache=False  # Avoid caching issues\n",
    "            )\n",
    "            onnx_model.save_pretrained(onnx_dir)\n",
    "            \n",
    "            # Find and rename the ONNX file\n",
    "            onnx_files = list(Path(onnx_dir).glob(\"*.onnx\"))\n",
    "            if onnx_files:\n",
    "                current_onnx = onnx_files[0]\n",
    "                target_path = Path(onnx_path)\n",
    "                if current_onnx != target_path:\n",
    "                    if target_path.exists():\n",
    "                        target_path.unlink()\n",
    "                    current_onnx.rename(target_path)\n",
    "                \n",
    "                print(f\"ONNX model exported to: {onnx_path}\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e1:\n",
    "            print(f\"Method 1 failed: {e1}, trying alternative...\")\n",
    "            \n",
    "            # Method 2: Direct torch.onnx export\n",
    "            from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "            import torch\n",
    "            \n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "            \n",
    "            # Create dummy input\n",
    "            dummy_text = \"sample input text\"\n",
    "            dummy_input = tokenizer(\n",
    "                dummy_text,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=128,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "            )\n",
    "            \n",
    "            # Export to ONNX\n",
    "            torch.onnx.export(\n",
    "                model,\n",
    "                (dummy_input['input_ids'], dummy_input['attention_mask']),\n",
    "                onnx_path,\n",
    "                export_params=True,\n",
    "                opset_version=14,  # Compatible with your onnxruntime 1.19.0\n",
    "                do_constant_folding=True,\n",
    "                input_names=['input_ids', 'attention_mask'],\n",
    "                output_names=['logits'],\n",
    "                dynamic_axes={\n",
    "                    'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "                    'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
    "                    'logits': {0: 'batch_size'}\n",
    "                },\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            print(f\"ONNX model exported to: {onnx_path}\")\n",
    "            return True\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"Import error: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"ONNX export failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e05eab",
   "metadata": {},
   "source": [
    "TENSORFLOW LITE EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_tflite_alternative(model_dir, tflite_path):\n",
    "    \"\"\"Alternative TFLite export method using TensorFlow 2.17\"\"\"\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "        \n",
    "        print(f\"Using TensorFlow {tf.__version__}\")\n",
    "        \n",
    "        # Load tokenizer to get proper input specs\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        \n",
    "        # Load as TensorFlow model from PyTorch checkpoint\n",
    "        tf_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            model_dir, \n",
    "            from_pt=True  # Correct parameter for transformers 4.44.2\n",
    "        )\n",
    "\n",
    "        # Create converter\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        \n",
    "        # Representative dataset for quantization\n",
    "        def representative_dataset():\n",
    "            sample_texts = [\n",
    "                \"hello world\", \"test message\", \"sample text\", \"another example\",\n",
    "                \"short\", \"this is a longer text for testing purposes\"\n",
    "            ]\n",
    "            \n",
    "            for text in sample_texts:\n",
    "                # Tokenize properly\n",
    "                inputs = tokenizer(\n",
    "                    text, \n",
    "                    return_tensors=\"tf\", \n",
    "                    max_length=128, \n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True\n",
    "                )\n",
    "                yield [inputs['input_ids'], inputs['attention_mask']]\n",
    "        \n",
    "        converter.representative_dataset = representative_dataset\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "        \n",
    "        # Convert\n",
    "        tflite_model = converter.convert()\n",
    "\n",
    "        # Save TFLite model\n",
    "        os.makedirs(os.path.dirname(tflite_path), exist_ok=True)\n",
    "        with open(tflite_path, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "\n",
    "        print(f\"TFLite model exported to: {tflite_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"TFLite export failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44831ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_tflite(model_dir, tflite_path):\n",
    "    \"\"\"Convert model to TensorFlow Lite using your installed packages\"\"\"\n",
    "    print(\"Converting model to TensorFlow Lite...\")\n",
    "\n",
    "    try:\n",
    "        # First try the direct TF method\n",
    "        success = export_tflite_alternative(model_dir, tflite_path)\n",
    "        if success:\n",
    "            return True\n",
    "            \n",
    "        # If that fails, try ONNX -> TF -> TFLite pipeline\n",
    "        print(\"Trying ONNX -> TensorFlow -> TFLite conversion...\")\n",
    "        \n",
    "        import onnx\n",
    "        import tensorflow as tf\n",
    "        from onnx_tf.backend import prepare\n",
    "        import tempfile\n",
    "        \n",
    "        # Create temporary ONNX file\n",
    "        temp_onnx = os.path.join(tempfile.gettempdir(), \"temp_model.onnx\")\n",
    "        \n",
    "        # Export to ONNX first\n",
    "        from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "        import torch\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        \n",
    "        dummy_input = tokenizer(\n",
    "            \"sample text\",\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        \n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            (dummy_input['input_ids'], dummy_input['attention_mask']),\n",
    "            temp_onnx,\n",
    "            export_params=True,\n",
    "            opset_version=12,  # Lower opset for better onnx-tf compatibility\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input_ids', 'attention_mask'],\n",
    "            output_names=['logits']\n",
    "        )\n",
    "        \n",
    "        # Load ONNX model and convert to TF\n",
    "        onnx_model = onnx.load(temp_onnx)\n",
    "        tf_rep = prepare(onnx_model)\n",
    "        \n",
    "        # Convert to TFLite\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            saved_model_path = os.path.join(temp_dir, \"saved_model\")\n",
    "            tf_rep.export_graph(saved_model_path)\n",
    "            \n",
    "            converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
    "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "            \n",
    "            tflite_model = converter.convert()\n",
    "            \n",
    "            # Save TFLite model\n",
    "            os.makedirs(os.path.dirname(tflite_path), exist_ok=True)\n",
    "            with open(tflite_path, \"wb\") as f:\n",
    "                f.write(tflite_model)\n",
    "            \n",
    "        # Cleanup\n",
    "        if os.path.exists(temp_onnx):\n",
    "            os.remove(temp_onnx)\n",
    "            \n",
    "        print(f\"TFLite model exported to: {tflite_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"TFLite export failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1eec33",
   "metadata": {},
   "source": [
    "MAIN TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main training and export pipeline\"\"\"\n",
    "    print(\"Starting NSFW Detection Model Training Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Configuration\n",
    "    MODEL_NAME = \"microsoft/xtremedistil-l6-h256-uncased\"\n",
    "    OUTPUT_DIR = \"models/xtremedistil-l6-h256-uncased\"\n",
    "    ONNX_PATH = \"models/nsfw_model.onnx\"\n",
    "    TFLITE_PATH = \"models/nsfw_model.tflite\"\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(\"metrics\", exist_ok=True)\n",
    "\n",
    "    # Step 1: Load dataset\n",
    "    df = load_dataset()\n",
    "\n",
    "    # Step 2: Split dataset\n",
    "    X_train, X_val, y_train, y_val = split_dataset(df)\n",
    "\n",
    "    # Step 3: Create tokenized datasets\n",
    "    train_dataset, val_dataset, tokenizer = create_tokenized_datasets(\n",
    "        X_train, X_val, y_train, y_val, MODEL_NAME\n",
    "    )\n",
    "\n",
    "    # Step 4: Train model\n",
    "    trainer, model = train_model(\n",
    "        train_dataset, val_dataset, tokenizer, MODEL_NAME, OUTPUT_DIR\n",
    "    )\n",
    "\n",
    "    # Step 5: Evaluate model (corrected function call)\n",
    "    accuracy, report = evaluate_model(trainer, y_val, OUTPUT_DIR)\n",
    "\n",
    "    # Step 6: Export to ONNX\n",
    "    onnx_success = export_to_onnx(OUTPUT_DIR, ONNX_PATH)\n",
    "\n",
    "    # Step 7: Export to TensorFlow Lite (pass model_dir correctly)\n",
    "    tflite_success = False\n",
    "    if onnx_success:\n",
    "        tflite_success = export_to_tflite(OUTPUT_DIR, TFLITE_PATH)\n",
    "    else:\n",
    "        # Try TFLite export even if ONNX failed\n",
    "        tflite_success = export_tflite_alternative(OUTPUT_DIR, TFLITE_PATH)\n",
    "\n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training and Export Summary:\")\n",
    "    print(f\" Model trained successfully - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{'' if onnx_success else ''} ONNX export: {'SUCCESS' if onnx_success else 'FAILED'}\")\n",
    "    print(f\"{'' if tflite_success else ''} TFLite export: {'SUCCESS' if tflite_success else 'FAILED'}\")\n",
    "    print(f\"\\nOutput files:\")\n",
    "    print(f\"  - Model: {OUTPUT_DIR}\")\n",
    "    print(f\"  - Metrics: metrics/metrics.txt\")\n",
    "    if onnx_success:\n",
    "        print(f\"  - ONNX: {ONNX_PATH}\")\n",
    "    if tflite_success:\n",
    "        print(f\"  - TFLite: {TFLITE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f77849",
   "metadata": {},
   "source": [
    "Sample CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abf20769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(model_dir, test_texts=None):\n",
    "    \"\"\"Test the trained model with sample texts\"\"\"\n",
    "    if test_texts is None:\n",
    "        test_texts = [\"hello world\", \"putang ina\"]\n",
    "    \n",
    "    print(\"\\nTesting trained model...\")\n",
    "    \n",
    "    try:\n",
    "        # Load model and tokenizer\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for text in test_texts:\n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                text, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=128,\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            # Predict\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "                confidence = predictions[0][predicted_class].item()\n",
    "            \n",
    "            labels = \"SAFE\" if predicted_class == 0 else \"NSFW\"\n",
    "            print(f\"Text: '{text}' -> {labels} (confidence: {confidence:.4f})\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Inference test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4544fa7b",
   "metadata": {},
   "source": [
    "PROGRAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0aeff701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NSFW Detection Model Training Pipeline\n",
      "============================================================\n",
      "Loading dataset from dataset.csv...\n",
      "Error: Dataset file 'dataset.csv' not found!\n",
      "Creating a sample dataset for demonstration...\n",
      "Sample dataset created and saved as 'dataset.csv'\n",
      "Splitting dataset: 20.0% for validation...\n",
      "Training set: 38 samples\n",
      "Validation set: 10 samples\n",
      "Loading tokenizer and creating tokenized datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:00<00:00, 503.59 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete!\n",
      "Initializing model for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/xtremedistil-l6-h256-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 3/15 [00:01<00:03,  3.05it/s]\n",
      " 20%|â–ˆâ–ˆ        | 3/15 [00:01<00:03,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6898426413536072, 'eval_accuracy': 0.6, 'eval_runtime': 0.0287, 'eval_samples_per_second': 348.515, 'eval_steps_per_second': 34.851, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:02<00:03,  2.39it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:02<00:03,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6873841881752014, 'eval_accuracy': 0.6, 'eval_runtime': 0.045, 'eval_samples_per_second': 222.204, 'eval_steps_per_second': 22.22, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:03<00:01,  3.02it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:03<00:01,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6850928068161011, 'eval_accuracy': 0.6, 'eval_runtime': 0.0188, 'eval_samples_per_second': 531.887, 'eval_steps_per_second': 53.189, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:04<00:02,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.69, 'grad_norm': 0.6976374983787537, 'learning_rate': 6.666666666666667e-06, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:04<00:00,  3.06it/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:04<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6833875775337219, 'eval_accuracy': 0.6, 'eval_runtime': 0.0182, 'eval_samples_per_second': 549.417, 'eval_steps_per_second': 54.942, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:05<00:00,  3.37it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:06<00:00,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6826910376548767, 'eval_accuracy': 0.6, 'eval_runtime': 0.016, 'eval_samples_per_second': 626.745, 'eval_steps_per_second': 62.675, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:06<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6.4692, 'train_samples_per_second': 29.37, 'train_steps_per_second': 2.319, 'train_loss': 0.689188273747762, 'epoch': 5.0}\n",
      "Saving model to models/xtremedistil-l6-h256-uncased...\n",
      "Evaluating model performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 500.39it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 509.26it/s]\n",
      "c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to metrics/metrics.txt\n",
      "Validation Accuracy: 0.6000\n",
      "Exporting model to ONNX format...\n",
      "Method 1 failed: ORTModel._from_transformers() got an unexpected keyword argument 'use_cache', trying alternative...\n",
      "ONNX model exported to: models/nsfw_model.onnx\n",
      "Converting model to TensorFlow Lite...\n",
      "Using TensorFlow 2.17.0\n",
      "TFLite export failed: ('Keyword argument not understood:', 'low_cpu_mem_usage')\n",
      "Trying ONNX -> TensorFlow -> TFLite conversion...\n",
      "TFLite export failed: No module named 'keras.src.engine'\n",
      "\n",
      "============================================================\n",
      "Training and Export Summary:\n",
      "âœ… Model trained successfully - Accuracy: 0.6000\n",
      "âœ… ONNX export: SUCCESS\n",
      "âŒ TFLite export: FAILED\n",
      "\n",
      "Output files:\n",
      "  - Model: models/xtremedistil-l6-h256-uncased\n",
      "  - Metrics: metrics/metrics.txt\n",
      "  - ONNX: models/nsfw_model.onnx\n",
      "\n",
      "Testing trained model...\n",
      "Text: 'hello world' -> SAFE (confidence: 0.5052)\n",
      "Text: 'putang ina' -> SAFE (confidence: 0.5021)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_19544\\2416132881.py\", line 13, in export_tflite_alternative\n",
      "    tf_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
      "  File \"c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2951, in from_pretrained\n",
      "    model = cls(config, *model_args, **model_kwargs)\n",
      "  File \"c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1701, in __init__\n",
      "    super().__init__(config, *inputs, **kwargs)\n",
      "  File \"c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1210, in __init__\n",
      "    super().__init__(*inputs, **kwargs)\n",
      "  File \"c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py\", line 204, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\tf_keras\\src\\utils\\generic_utils.py\", line 513, in validate_kwargs\n",
      "    raise TypeError(error_message, kwarg)\n",
      "TypeError: ('Keyword argument not understood:', 'low_cpu_mem_usage')\n",
      "c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.17.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_19544\\1142464221.py\", line 16, in export_to_tflite\n",
      "    from onnx_tf.backend import prepare\n",
      "  File \"c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\onnx_tf\\__init__.py\", line 1, in <module>\n",
      "    from . import backend\n",
      "  File \"c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\onnx_tf\\backend.py\", line 28, in <module>\n",
      "    from onnx_tf.common.handler_helper import get_all_backend_handlers\n",
      "  File \"c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\onnx_tf\\common\\handler_helper.py\", line 3, in <module>\n",
      "    from onnx_tf.handlers.backend import *  # noqa\n",
      "  File \"c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\onnx_tf\\handlers\\backend\\hardmax.py\", line 3, in <module>\n",
      "    import tensorflow_addons as tfa\n",
      "  File \"c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\tensorflow_addons\\__init__.py\", line 23, in <module>\n",
      "    from tensorflow_addons import activations\n",
      "  File \"c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\tensorflow_addons\\activations\\__init__.py\", line 17, in <module>\n",
      "    from tensorflow_addons.activations.gelu import gelu\n",
      "  File \"c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\tensorflow_addons\\activations\\gelu.py\", line 19, in <module>\n",
      "    from tensorflow_addons.utils.types import TensorLike\n",
      "  File \"c:\\Users\\USER\\miniconda3\\envs\\ThesisBert\\lib\\site-packages\\tensorflow_addons\\utils\\types.py\", line 29, in <module>\n",
      "    from keras.src.engine import keras_tensor\n",
      "ModuleNotFoundError: No module named 'keras.src.engine'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Run the main pipeline\n",
    "        main()\n",
    "        \n",
    "        # Optional: Test inference\n",
    "        test_inference(\"models/xtremedistil-l6-h256-uncased\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThesisBert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
