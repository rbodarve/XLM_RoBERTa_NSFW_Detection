{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ywa5gS3H-KIx"
      },
      "outputs": [],
      "source": [
        "#import shutil\n",
        "#shutil.rmtree('/content/sample_data', ignore_errors=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XLM-RoBERTa"
      ],
      "metadata": {
        "id": "resoeJaD-mKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_recall_fscore_support\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    XLMRobertaTokenizer,\n",
        "    XLMRobertaForSequenceClassification\n",
        ")\n",
        "from datasets import Dataset\n",
        "import tempfile\n",
        "import subprocess\n",
        "import sys"
      ],
      "metadata": {
        "id": "40U2x5_g-t9u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORT LIBRARIES AND SETUP"
      ],
      "metadata": {
        "id": "rr3TPcBg-wNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def install_requirements():\n",
        "    required_packages = [\n",
        "        'transformers[torch]',\n",
        "        'datasets',\n",
        "        'torch',\n",
        "        'pandas',\n",
        "        'scikit-learn',\n",
        "        'onnx',\n",
        "        'onnxruntime',\n",
        "        'optimum[onnxruntime]',  # ← This enables ORTModelForSequenceClassification\n",
        "        'tensorflow',\n",
        "    ]\n",
        "\n",
        "    for package in required_packages:\n",
        "        try:\n",
        "            __import__(package.split('[')[0])\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])"
      ],
      "metadata": {
        "id": "_02P0W8w-zZa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Checking and installing dependencies...\")\n",
        "install_requirements()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH7ujfkr-1QN",
        "outputId": "7d559513-4a57-4e53-9a8d-429fca0cceb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking and installing dependencies...\n",
            "Installing scikit-learn...\n",
            "Installing onnx...\n",
            "Installing onnxruntime...\n",
            "Installing optimum[onnxruntime]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "    from optimum.onnxruntime.configuration import OptimizationConfig\n",
        "    ONNX_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Optimum ONNX Runtime not available, ONNX export will be limited\")\n",
        "    ONNX_AVAILABLE = False"
      ],
      "metadata": {
        "id": "4Vl4PzZ8HPMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"All dependencies loaded successfully!\")"
      ],
      "metadata": {
        "id": "pihPjHWb-4a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD AND PREPARE DATASET"
      ],
      "metadata": {
        "id": "n7LbJgAk-7dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(csv_path='dataset.csv'):\n",
        "    print(f\"Loading multilingual dataset from {csv_path}...\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
        "\n",
        "        # Validate required columns\n",
        "        required_cols = ['text', 'label']\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        # Clean and validate data\n",
        "        df = df.dropna(subset=['text', 'label'])\n",
        "        df['text'] = df['text'].astype(str)\n",
        "        df['label'] = df['label'].astype(int)\n",
        "\n",
        "        # Validate and normalize labels\n",
        "        unique_labels = sorted(df['label'].unique())\n",
        "        num_labels = len(unique_labels)\n",
        "\n",
        "        # Ensure labels are sequential starting from 0\n",
        "        if unique_labels != list(range(num_labels)):\n",
        "            print(\"Warning: Labels are not sequential starting from 0. Remapping...\")\n",
        "            label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}\n",
        "            df['label'] = df['label'].map(label_mapping)\n",
        "            print(f\"Label mapping: {label_mapping}\")\n",
        "\n",
        "        print(f\"Dataset validation complete. Clean shape: {df.shape}\")\n",
        "        print(f\"Number of classes: {num_labels}\")\n",
        "        print(f\"Label distribution:\\n{df['label'].value_counts().sort_index()}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Dataset file '{csv_path}' not found!\")\n",
        "        print(\"Creating a sample multilingual dataset for demonstration...\")\n",
        "        return create_sample_dataset()"
      ],
      "metadata": {
        "id": "bzVyf_N1--cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_sample_dataset():\n",
        "    sample_data = {\n",
        "        'text': [\n",
        "            # English samples (labels 0-2)\n",
        "            'This product is absolutely amazing and works perfectly!',\n",
        "            'Great quality and excellent customer service.',\n",
        "            'I highly recommend this to everyone, outstanding performance.',\n",
        "            'Love the design and functionality, very satisfied.',\n",
        "            'Exceptional value for money, exceeded expectations.',\n",
        "\n",
        "            'The product is okay, meets basic requirements.',\n",
        "            'Average quality, nothing special but works fine.',\n",
        "            'Standard features, typical for this price range.',\n",
        "            'It works as described, no major complaints.',\n",
        "            'Decent product, could be better but acceptable.',\n",
        "\n",
        "            'Terrible quality, completely disappointed with purchase.',\n",
        "            'Worst customer service experience ever encountered.',\n",
        "            'Product broke after just one day of use.',\n",
        "            'Would not recommend, waste of money and time.',\n",
        "            'Poor build quality and unreliable performance.',\n",
        "\n",
        "            # Spanish samples\n",
        "            'Este producto es absolutamente increíble, funciona perfectamente.',\n",
        "            'Excelente calidad y servicio al cliente excepcional.',\n",
        "            'Lo recomiendo mucho, rendimiento extraordinario y confiable.',\n",
        "            'Me encanta el diseño, muy satisfecho con la compra.',\n",
        "            'Valor excepcional, superó todas mis expectativas completamente.',\n",
        "\n",
        "            'El producto está bien, cumple con lo básico necesario.',\n",
        "            'Calidad promedio, nada especial pero funciona correctamente.',\n",
        "            'Características estándar, típico para este rango de precios.',\n",
        "            'Funciona como se describe, sin quejas importantes.',\n",
        "            'Producto decente, podría ser mejor pero aceptable.',\n",
        "\n",
        "            'Calidad terrible, completamente decepcionado con la compra realizada.',\n",
        "            'La peor experiencia de servicio al cliente jamás experimentada.',\n",
        "            'El producto se rompió después de solo un día.',\n",
        "            'No lo recomendaría, pérdida de dinero y tiempo.',\n",
        "            'Mala calidad de construcción y rendimiento poco confiable.',\n",
        "\n",
        "            # Tagalog samples\n",
        "            'Napakaganda ng produktong ito, perpektong gumagana!',\n",
        "            'Napakahusay ng kalidad at serbisyo sa customer.',\n",
        "            'Highly recommended ko ito sa lahat, outstanding performance.',\n",
        "            'Love ko ang design, very satisfied sa purchase.',\n",
        "            'Exceptional value for money, sobra sa expectations.',\n",
        "\n",
        "            'Okay lang ang produkto, nakakameet ng basic requirements.',\n",
        "            'Average quality, walang special pero gumagana naman.',\n",
        "            'Standard features, typical sa price range na ito.',\n",
        "            'Gumagana naman as described, walang major complaints.',\n",
        "            'Decent product, pwede pa mas better pero acceptable.',\n",
        "\n",
        "            'Napakasama ng quality, disappointed ako sa purchase.',\n",
        "            'Worst customer service experience na naranasan ko.',\n",
        "            'Nasira ang produkto after one day lang.',\n",
        "            'Hindi ko irerekumenda, sayang ang pera at oras.',\n",
        "            'Pangit ng build quality at hindi reliable performance.',\n",
        "\n",
        "            # French samples\n",
        "            'Ce produit est absolument incroyable, fonctionne parfaitement bien!',\n",
        "            'Excellente qualité et service client exceptionnel et professionnel.',\n",
        "            'Je le recommande vivement, performances extraordinaires et fiables.',\n",
        "            'J\\'adore le design, très satisfait de cet achat.',\n",
        "            'Valeur exceptionnelle, a dépassé toutes mes attentes complètement.',\n",
        "\n",
        "            'Le produit est correct, répond aux exigences de base.',\n",
        "            'Qualité moyenne, rien de spécial mais fonctionne bien.',\n",
        "            'Fonctionnalités standard, typique pour cette gamme de prix.',\n",
        "            'Fonctionne comme décrit, pas de plaintes majeures importantes.',\n",
        "            'Produit décent, pourrait être mieux mais acceptable globalement.',\n",
        "\n",
        "            'Qualité terrible, complètement déçu de cet achat récent.',\n",
        "            'La pire expérience de service client jamais vécue.',\n",
        "            'Le produit s\\'est cassé après seulement une journée.',\n",
        "            'Je ne le recommanderais pas, perte d\\'argent.',\n",
        "            'Mauvaise qualité de construction et performances peu fiables.',\n",
        "\n",
        "            # German samples\n",
        "            'Dieses Produkt ist absolut fantastisch und funktioniert perfekt!',\n",
        "            'Ausgezeichnete Qualität und hervorragender Kundenservice immer.',\n",
        "            'Ich empfehle es sehr, außergewöhnliche Leistung und Zuverlässigkeit.',\n",
        "            'Ich liebe das Design, sehr zufrieden mit dem Kauf.',\n",
        "            'Außergewöhnlicher Wert, hat alle Erwartungen vollständig übertroffen.',\n",
        "\n",
        "            'Das Produkt ist okay, erfüllt die grundlegenden Anforderungen gut.',\n",
        "            'Durchschnittliche Qualität, nichts Besonderes aber funktioniert einwandfrei.',\n",
        "            'Standard-Features, typisch für diese Preisklasse und Kategorie.',\n",
        "            'Funktioniert wie beschrieben, keine größeren Beschwerden vorhanden.',\n",
        "            'Anständiges Produkt, könnte besser sein aber akzeptabel.',\n",
        "\n",
        "            'Schreckliche Qualität, völlig enttäuscht von diesem Kauf heute.',\n",
        "            'Die schlechteste Kundenservice-Erfahrung, die ich je gemacht habe.',\n",
        "            'Das Produkt ging nach nur einem Tag kaputt.',\n",
        "            'Würde ich nicht empfehlen, Geld- und Zeitverschwendung.',\n",
        "            'Schlechte Bauqualität und unzuverlässige Leistung durchgehend.',\n",
        "        ],\n",
        "        'label': (\n",
        "            # English: 5 positive, 5 neutral, 5 negative\n",
        "            [2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] +\n",
        "            # Spanish: 5 positive, 5 neutral, 5 negative\n",
        "            [2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] +\n",
        "            # Tagalog: 5 positive, 5 neutral, 5 negative\n",
        "            [2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] +\n",
        "            # French: 5 positive, 5 neutral, 5 negative\n",
        "            [2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] +\n",
        "            # German: 5 positive, 5 neutral, 5 negative\n",
        "            [2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
        "        )\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(sample_data)\n",
        "    df.to_csv('dataset.csv', index=False)\n",
        "    print(\"Sample multilingual sentiment dataset created and saved as 'dataset.csv'\")\n",
        "    print(\"Dataset includes English, Spanish, Tagalog, French, and German samples\")\n",
        "    print(\"Labels: 0=Negative, 1=Neutral, 2=Positive\")\n",
        "    print(f\"Total samples: {len(df)} across 5 languages\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "0a5WSdnJ_Con"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(df, test_size=0.2, random_state=42):\n",
        "    print(f\"Splitting dataset: {test_size*100}% for validation...\")\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        df['text'].tolist(),\n",
        "        df['label'].tolist(),\n",
        "        test_size=test_size,\n",
        "        random_state=random_state,\n",
        "        stratify=df['label']\n",
        "    )\n",
        "\n",
        "    print(f\"Training set: {len(X_train)} samples\")\n",
        "    print(f\"Validation set: {len(X_val)} samples\")\n",
        "\n",
        "    return X_train, X_val, y_train, y_val"
      ],
      "metadata": {
        "id": "n8nadu-E_fXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOKENIZATION AND PREPROCESSING"
      ],
      "metadata": {
        "id": "t2dw7FWN_iHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tokenized_datasets(X_train, X_val, y_train, y_val, model_name):\n",
        "    \"\"\"Tokenize the datasets using the XLM-RoBERTa tokenizer\"\"\"\n",
        "    print(\"Loading XLM-RoBERTa tokenizer and creating tokenized datasets...\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # XLM-RoBERTa uses SentencePiece tokenizer, similar to RoBERTa\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(f\"Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
        "    print(f\"Model max length: {tokenizer.model_max_length}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = Dataset.from_dict({\n",
        "        'text': X_train,\n",
        "        'labels': y_train\n",
        "    })\n",
        "\n",
        "    val_dataset = Dataset.from_dict({\n",
        "        'text': X_val,\n",
        "        'labels': y_val\n",
        "    })\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples['text'],\n",
        "            truncation=True,\n",
        "            padding=False,  # Will be handled by data collator\n",
        "            max_length=256  # Good for multilingual text processing\n",
        "        )\n",
        "\n",
        "    # Tokenize datasets\n",
        "    print(\"Tokenizing training dataset...\")\n",
        "    train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    print(\"Tokenizing validation dataset...\")\n",
        "    val_tokenized = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    print(\"Tokenization complete!\")\n",
        "    print(f\"Sample tokenized text length: {len(train_tokenized[0]['input_ids'])}\")\n",
        "\n",
        "    return train_tokenized, val_tokenized, tokenizer"
      ],
      "metadata": {
        "id": "YB_9DFNf_mr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL TRAINING"
      ],
      "metadata": {
        "id": "uDBa6Wqd_pVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_dataset, val_dataset, tokenizer, model_name, output_dir, num_labels):\n",
        "    \"\"\"Train the XLM-RoBERTa model for multilingual classification\"\"\"\n",
        "    print(\"Initializing XLM-RoBERTa model for multilingual classification training...\")\n",
        "\n",
        "    # Load model with appropriate number of labels\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels,\n",
        "        problem_type=\"single_label_classification\"\n",
        "    )\n",
        "\n",
        "    print(f\"Model loaded with {model.num_parameters():,} parameters\")\n",
        "    print(f\"Model architecture: {model.config.hidden_size} hidden, {model.config.num_hidden_layers} layers\")\n",
        "\n",
        "    # Ensure model uses the correct pad_token_id\n",
        "    if tokenizer.pad_token_id is not None:\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # Training arguments optimized for XLM-RoBERTa\n",
        "    training_args_dict = {\n",
        "        'output_dir': output_dir,\n",
        "        'num_train_epochs': 3,  # Standard for fine-tuning large models\n",
        "        'per_device_train_batch_size': 8,  # Conservative for 270M parameter model\n",
        "        'per_device_eval_batch_size': 8,\n",
        "        'learning_rate': 2e-5,  # Lower learning rate for large pre-trained model\n",
        "        'weight_decay': 0.01,\n",
        "        'warmup_ratio': 0.06,  # 6% warmup recommended for RoBERTa\n",
        "        'logging_dir': f'{output_dir}/logs',\n",
        "        'logging_steps': 25,\n",
        "        'save_total_limit': 2,\n",
        "        'load_best_model_at_end': True,\n",
        "        'metric_for_best_model': \"eval_f1_weighted\",\n",
        "        'greater_is_better': True,\n",
        "        'report_to': [],\n",
        "        'seed': 42,\n",
        "        'dataloader_num_workers': 0,\n",
        "        'remove_unused_columns': True,\n",
        "        'fp16': True,  # Mixed precision for efficiency\n",
        "        'dataloader_pin_memory': False,\n",
        "        'gradient_checkpointing': True,  # Save memory for large model\n",
        "        'max_grad_norm': 1.0,  # Gradient clipping\n",
        "        'lr_scheduler_type': 'cosine',  # Cosine learning rate schedule\n",
        "    }\n",
        "\n",
        "    # Add version-specific parameters\n",
        "    if hasattr(TrainingArguments, 'eval_strategy'):\n",
        "        training_args_dict['eval_strategy'] = \"steps\"\n",
        "        training_args_dict['eval_steps'] = 100\n",
        "        training_args_dict['save_strategy'] = \"steps\"\n",
        "        training_args_dict['save_steps'] = 100\n",
        "    else:\n",
        "        training_args_dict['evaluation_strategy'] = \"steps\"\n",
        "        training_args_dict['eval_steps'] = 100\n",
        "        training_args_dict['save_strategy'] = \"steps\"\n",
        "        training_args_dict['save_steps'] = 100\n",
        "\n",
        "    training_args = TrainingArguments(**training_args_dict)\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "\n",
        "        # Handle different prediction formats\n",
        "        if isinstance(predictions, tuple):\n",
        "            predictions = predictions[0]\n",
        "\n",
        "        # Convert to numpy array if it's a tensor\n",
        "        if hasattr(predictions, 'numpy'):\n",
        "            predictions = predictions.numpy()\n",
        "\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "        # Calculate comprehensive metrics\n",
        "        accuracy = accuracy_score(labels, predictions)\n",
        "        f1_macro = f1_score(labels, predictions, average='macro')\n",
        "        f1_weighted = f1_score(labels, predictions, average='weighted')\n",
        "        precision_macro = precision_recall_fscore_support(labels, predictions, average='macro')[0]\n",
        "        recall_macro = precision_recall_fscore_support(labels, predictions, average='macro')[1]\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'f1_macro': f1_macro,\n",
        "            'f1_weighted': f1_weighted,\n",
        "            'precision_macro': precision_macro,\n",
        "            'recall_macro': recall_macro,\n",
        "        }\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the best model\n",
        "    print(f\"Saving model to {output_dir}...\")\n",
        "    trainer.save_model()\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    return trainer, model"
      ],
      "metadata": {
        "id": "D2_0ZksZ_u0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL EVALUATION"
      ],
      "metadata": {
        "id": "IaRXZ7Dp_w9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(trainer, X_val, y_val, output_dir, num_labels):\n",
        "    print(\"Evaluating XLM-RoBERTa model performance...\")\n",
        "\n",
        "    # Get predictions\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    # Get detailed predictions for classification report\n",
        "    predictions = trainer.predict(trainer.eval_dataset)\n",
        "\n",
        "    # Extract predictions from the prediction object\n",
        "    if hasattr(predictions, 'predictions'):\n",
        "        preds = predictions.predictions\n",
        "    else:\n",
        "        preds = predictions[0]\n",
        "\n",
        "    # Convert to numpy array if it's a tensor\n",
        "    if hasattr(preds, 'numpy'):\n",
        "        preds = preds.numpy()\n",
        "\n",
        "    y_pred = np.argmax(preds, axis=1)\n",
        "\n",
        "    # Generate classification report\n",
        "    if num_labels == 2:\n",
        "        target_names = ['NSFW', 'Safe']\n",
        "    elif num_labels == 3:\n",
        "        # Map: 0=NSFW, 1=Safe, 2=Safe (combining neutral and positive as Safe)\n",
        "        target_names = ['NSFW', 'Safe', 'Safe']\n",
        "        # Remap predictions: combine classes 1 and 2 into 'Safe'\n",
        "        y_val_binary = [0 if label == 0 else 1 for label in y_val]\n",
        "        y_pred_binary = [0 if pred == 0 else 1 for pred in y_pred]\n",
        "\n",
        "        # Use binary classification for final report\n",
        "        report = classification_report(\n",
        "            y_val_binary,\n",
        "            y_pred_binary,\n",
        "            target_names=['NSFW', 'Safe'],\n",
        "            digits=4\n",
        "        )\n",
        "\n",
        "        # Calculate binary metrics\n",
        "        accuracy = accuracy_score(y_val_binary, y_pred_binary)\n",
        "        f1_macro = f1_score(y_val_binary, y_pred_binary, average='macro')\n",
        "        f1_weighted = f1_score(y_val_binary, y_pred_binary, average='weighted')\n",
        "        precision_macro = precision_recall_fscore_support(y_val_binary, y_pred_binary, average='macro')[0]\n",
        "        recall_macro = precision_recall_fscore_support(y_val_binary, y_pred_binary, average='macro')[1]\n",
        "    else:\n",
        "        target_names = [f'Class_{i}' for i in range(num_labels)]\n",
        "        report = classification_report(\n",
        "            y_val,\n",
        "            y_pred,\n",
        "            target_names=target_names,\n",
        "            digits=4\n",
        "        )\n",
        "\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        f1_macro = f1_score(y_val, y_pred, average='macro')\n",
        "        f1_weighted = f1_score(y_val, y_pred, average='weighted')\n",
        "        precision_macro = precision_recall_fscore_support(y_val, y_pred, average='macro')[0]\n",
        "        recall_macro = precision_recall_fscore_support(y_val, y_pred, average='macro')[1]\n",
        "\n",
        "    # Prepare metrics text\n",
        "    task_description = \"Multilingual Content Safety Classification (NSFW vs Safe)\" if num_labels <= 3 else f\"Multilingual Text Classification ({num_labels} classes)\"\n",
        "\n",
        "    metrics_text = f\"\"\"XLM-RoBERTa Content Safety Classification Model Evaluation Results\n",
        "{'='*75}\n",
        "\n",
        "Model: FacebookAI/xlm-roberta-base\n",
        "Task: {task_description}\n",
        "Architecture: 12 layers, 768 hidden units, 270M parameters\n",
        "Languages: Supports 100+ languages including major world languages\n",
        "\n",
        "Performance Metrics:\n",
        "{'-'*40}\n",
        "Accuracy: {accuracy:.4f}\n",
        "F1-Score (Macro): {f1_macro:.4f}\n",
        "F1-Score (Weighted): {f1_weighted:.4f}\n",
        "Precision (Macro): {precision_macro:.4f}\n",
        "Recall (Macro): {recall_macro:.4f}\n",
        "\n",
        "Classification Report:\n",
        "{report}\n",
        "\n",
        "Training Results:\n",
        "{'-'*40}\n",
        "\"\"\"\n",
        "\n",
        "    for key, value in eval_results.items():\n",
        "        if isinstance(value, (int, float)):\n",
        "            metrics_text += f\"{key}: {value:.4f}\\n\"\n",
        "\n",
        "    # Save metrics\n",
        "    os.makedirs('metrics', exist_ok=True)\n",
        "    metrics_path = 'metrics/xlm_roberta_metrics.txt'\n",
        "\n",
        "    with open(metrics_path, 'w') as f:\n",
        "        f.write(metrics_text)\n",
        "\n",
        "    print(f\"Metrics saved to {metrics_path}\")\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n",
        "    print(f\"F1-Score (Weighted): {f1_weighted:.4f}\")\n",
        "\n",
        "    return accuracy, report"
      ],
      "metadata": {
        "id": "dgxUhFqe_0b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ONNX EXPORT"
      ],
      "metadata": {
        "id": "5PtHDRCK_3WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_to_onnx(model_dir, onnx_path):\n",
        "    print(\"Exporting XLM-RoBERTa model to ONNX format...\")\n",
        "\n",
        "    try:\n",
        "        # Load the trained model\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "        # Fix: Load tokenizer from original model name instead of saved directory\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
        "\n",
        "        # Create dummy input with multilingual sample\n",
        "        dummy_input = tokenizer(\n",
        "            \"This is a multilingual text sample for ONNX export testing\",\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=256,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        # Export to ONNX with optimization for XLM-RoBERTa\n",
        "        os.makedirs(os.path.dirname(onnx_path), exist_ok=True)\n",
        "        torch.onnx.export(\n",
        "            model,\n",
        "            tuple(dummy_input.values()),\n",
        "            onnx_path,\n",
        "            export_params=True,\n",
        "            opset_version=14,\n",
        "            do_constant_folding=True,\n",
        "            input_names=['input_ids', 'attention_mask'],\n",
        "            output_names=['logits'],\n",
        "            dynamic_axes={\n",
        "                'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
        "                'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
        "                'logits': {0: 'batch_size'}\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(f\"ONNX model exported to: {onnx_path}\")\n",
        "\n",
        "        # Get model size\n",
        "        model_size = os.path.getsize(onnx_path) / (1024 * 1024)\n",
        "        print(f\"ONNX model size: {model_size:.2f} MB\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ONNX export failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False"
      ],
      "metadata": {
        "id": "Nh7UnAAD_7p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TENSORFLOW LITE EXPORT"
      ],
      "metadata": {
        "id": "XGdZ7Vl7_-tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_to_tflite_from_pt(model_dir, tflite_path):\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "        print(\"Converting XLM-RoBERTa PyTorch model to TensorFlow...\")\n",
        "\n",
        "        # Load and convert to TensorFlow\n",
        "        tf_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "            model_dir,\n",
        "            from_pt=True\n",
        "        )\n",
        "\n",
        "        # Save as TensorFlow SavedModel\n",
        "        tf_saved_model_dir = os.path.join(model_dir, \"tf_saved_model\")\n",
        "        tf.saved_model.save(tf_model, tf_saved_model_dir)\n",
        "        print(f\"Saved intermediate TensorFlow model to {tf_saved_model_dir}\")\n",
        "\n",
        "        # Convert to TFLite with optimizations\n",
        "        converter = tf.lite.TFLiteConverter.from_saved_model(tf_saved_model_dir)\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "        # Additional optimizations for mobile deployment\n",
        "        converter.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "        tflite_model = converter.convert()\n",
        "\n",
        "        # Save TFLite model\n",
        "        os.makedirs(os.path.dirname(tflite_path), exist_ok=True)\n",
        "        with open(tflite_path, \"wb\") as f:\n",
        "            f.write(tflite_model)\n",
        "\n",
        "        # Get model size\n",
        "        model_size = os.path.getsize(tflite_path) / (1024 * 1024)\n",
        "        print(f\"TFLite model successfully exported to: {tflite_path}\")\n",
        "        print(f\"TFLite model size: {model_size:.2f} MB\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"TensorFlow Lite export failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False"
      ],
      "metadata": {
        "id": "a5Exlx7tABgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN"
      ],
      "metadata": {
        "id": "S-2To_L0ACRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"Starting XLM-RoBERTa Multilingual Text Classification Pipeline\")\n",
        "    print(\"=\"*75)\n",
        "\n",
        "    # Configuration - Updated to use XLM-RoBERTa model\n",
        "    MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
        "    OUTPUT_DIR = \"models/xlm_roberta_classification\"\n",
        "    ONNX_PATH = \"models/xlm_roberta_classification_model.onnx\"\n",
        "    TFLITE_PATH = \"models/xlm_roberta_classification_model.tflite\"\n",
        "\n",
        "    # Create output directories\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "    os.makedirs(\"metrics\", exist_ok=True)\n",
        "\n",
        "    print(f\"Using model: {MODEL_NAME}\")\n",
        "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "    # Step 1: Load dataset\n",
        "    df = load_dataset()\n",
        "    num_labels = len(df['label'].unique())\n",
        "\n",
        "    # Step 2: Split dataset\n",
        "    X_train, X_val, y_train, y_val = split_dataset(df)\n",
        "\n",
        "    # Step 3: Create tokenized datasets\n",
        "    train_dataset, val_dataset, tokenizer = create_tokenized_datasets(\n",
        "        X_train, X_val, y_train, y_val, MODEL_NAME\n",
        "    )\n",
        "\n",
        "    # Step 4: Train model\n",
        "    trainer, model = train_model(\n",
        "        train_dataset, val_dataset, tokenizer, MODEL_NAME, OUTPUT_DIR, num_labels\n",
        "    )\n",
        "\n",
        "    # Step 5: Evaluate model\n",
        "    accuracy, report = evaluate_model(trainer, X_val, y_val, OUTPUT_DIR, num_labels)\n",
        "\n",
        "    # Step 6: Export to ONNX\n",
        "    onnx_success = export_to_onnx(OUTPUT_DIR, ONNX_PATH)\n",
        "\n",
        "    # Step 7: Export to TFLite\n",
        "    tflite_success = export_to_tflite_from_pt(OUTPUT_DIR, TFLITE_PATH)\n",
        "\n",
        "    # Final output\n",
        "    print(\"\\n\" + \"=\"*75)\n",
        "    print(\"XLM-RoBERTa Multilingual Classification Training Complete!\")\n",
        "\n",
        "    if onnx_success:\n",
        "        print(f\"✅ ONNX model: {ONNX_PATH}\")\n",
        "    else:\n",
        "        print(\"❌ ONNX export: FAILED\")\n",
        "\n",
        "    if tflite_success:\n",
        "        print(f\"✅ TFLite model: {TFLITE_PATH}\")\n",
        "    else:\n",
        "        print(\"❌ TFLite export: FAILED\")\n",
        "\n",
        "    print(f\"\\nModel checkpoints: {OUTPUT_DIR}\")\n",
        "    print(f\"Metrics: metrics/xlm_roberta_metrics.txt\")\n",
        "    print(f\"Final validation accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "jf5Dv-hnAIG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INFERENCE"
      ],
      "metadata": {
        "id": "nWbaEcFsAIvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_inference(model_dir, test_texts=None):\n",
        "    if test_texts is None:\n",
        "        test_texts = [\n",
        "            # English samples\n",
        "            \"I absolutely love this amazing product, it's fantastic!\",\n",
        "            \"The product is okay, nothing particularly special about it.\",\n",
        "            \"Terrible quality, I'm completely disappointed with this purchase.\",\n",
        "\n",
        "            # Spanish samples\n",
        "            \"Este producto es absolutamente increÃ­ble, me encanta mucho.\",\n",
        "            \"El producto estÃ¡ bien, nada especialmente notable o destacable.\",\n",
        "            \"Calidad horrible, estoy completamente decepcionado con la compra.\",\n",
        "\n",
        "            # Tagalog samples\n",
        "            \"Napakaganda ng produktong ito, sobrang satisfied ako dito!\",\n",
        "            \"Okay lang naman ang produkto, walang special na features.\",\n",
        "            \"Sobrang pangit ng quality, disappointed ako sa purchase na ito.\",\n",
        "\n",
        "            # French samples\n",
        "            \"J'adore absolument ce produit, il est vraiment fantastique!\",\n",
        "            \"Le produit est correct, rien de particuliÃ¨rement remarquable vraiment.\",\n",
        "            \"QualitÃ© horrible, je suis complÃ¨tement dÃ©Ã§u de cet achat rÃ©cent.\",\n",
        "\n",
        "            # German samples\n",
        "            \"Ich liebe dieses Produkt absolut, es ist wirklich fantastisch!\",\n",
        "            \"Das Produkt ist okay, nichts besonders Bemerkenswertes daran wirklich.\",\n",
        "            \"Schreckliche QualitÃ¤t, ich bin vÃ¶llig enttÃ¤uscht von diesem Kauf.\",\n",
        "\n",
        "            # Italian samples\n",
        "            \"Amo assolutamente questo prodotto, Ã¨ davvero fantastico e perfetto!\",\n",
        "            \"Il prodotto va bene, niente di particolarmente notevole o speciale.\",\n",
        "            \"QualitÃ  orribile, sono completamente deluso da questo acquisto recente.\",\n",
        "\n",
        "            # Portuguese samples\n",
        "            \"Eu amo absolutamente este produto, Ã© realmente fantÃ¡stico e perfeito!\",\n",
        "            \"O produto estÃ¡ bem, nada particularmente notÃ¡vel ou especial mesmo.\",\n",
        "            \"Qualidade horrÃ­vel, estou completamente decepcionado com esta compra.\"\n",
        "        ]\n",
        "\n",
        "    print(\"\\nTesting trained XLM-RoBERTa multilingual model...\")\n",
        "\n",
        "    try:\n",
        "        # Load model and tokenizer\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "        # Fix: Load tokenizer from original model name instead of saved directory\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # Determine label names based on number of labels\n",
        "        num_labels = model.config.num_labels\n",
        "        if num_labels == 2:\n",
        "            label_names = [\"NSFW\", \"Safe\"]\n",
        "        elif num_labels == 3:\n",
        "            label_names = [\"NSFW\", \"Safe\", \"Safe\"]  # 0=NSFW, 1=Safe, 2=Safe\n",
        "        else:\n",
        "            label_names = [f\"Class_{i}\" for i in range(num_labels)]\n",
        "\n",
        "        language_names = [\"English\", \"Spanish\", \"Tagalog\", \"French\", \"German\", \"Italian\", \"Portuguese\"]\n",
        "\n",
        "        for i, text in enumerate(test_texts):\n",
        "            # Determine language\n",
        "            lang_idx = i // 3  # 3 samples per language\n",
        "            language = language_names[lang_idx] if lang_idx < len(language_names) else \"Unknown\"\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=256,\n",
        "                padding=True\n",
        "            )\n",
        "\n",
        "            # Predict\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "                predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "                confidence = predictions[0][predicted_class].item()\n",
        "\n",
        "            # Map predictions to labels for display\n",
        "            if num_labels == 3:\n",
        "                # Convert 3-class to binary for display\n",
        "                display_class = 0 if predicted_class == 0 else 1\n",
        "                display_label = \"NSFW\" if display_class == 0 else \"Safe\"\n",
        "                # Calculate combined Safe confidence for classes 1 and 2\n",
        "                if predicted_class == 0:\n",
        "                    display_confidence = confidence\n",
        "                else:\n",
        "                    display_confidence = (predictions[0][1] + predictions[0][2]).item()\n",
        "            else:\n",
        "                display_label = label_names[predicted_class] if predicted_class < len(label_names) else f\"Class_{predicted_class}\"\n",
        "                display_confidence = confidence\n",
        "\n",
        "            print(f\"[{language}] Text: '{text[:80]}{'...' if len(text) > 80 else ''}'\")\n",
        "            print(f\"  -> {display_label} (confidence: {display_confidence:.4f})\")\n",
        "\n",
        "            # Show probabilities\n",
        "            if num_labels == 3:\n",
        "                nsfw_prob = predictions[0][0].item()\n",
        "                safe_prob = (predictions[0][1] + predictions[0][2]).item()\n",
        "                print(f\"  Probabilities: NSFW={nsfw_prob:.3f}, Safe={safe_prob:.3f}\")\n",
        "            else:\n",
        "                probs_str = \", \".join([f\"{label_names[j] if j < len(label_names) else f'Class_{j}'}={predictions[0][j].item():.3f}\"\n",
        "                                     for j in range(num_labels)])\n",
        "                print(f\"  Probabilities: {probs_str}\")\n",
        "            print()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Inference test failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "h4L0c-BpAMb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROGRAM EXECUTION"
      ],
      "metadata": {
        "id": "yMtlQnqTAN2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Run the main pipeline\n",
        "        main()\n",
        "\n",
        "        # Optional: Test inference\n",
        "        test_inference(\"models/xlm_roberta_classification\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nTraining interrupted by user.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\nProgram execution completed.\")"
      ],
      "metadata": {
        "id": "fpFqxZNeAP5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create zip of entire content folder\n",
        "!zip -r /content/colab_content_XLM.zip /content/\n",
        "\n",
        "# Download the zip\n",
        "from google.colab import files\n",
        "files.download('/content/colab_content_XLM.zip')"
      ],
      "metadata": {
        "id": "02QD6vGtLJyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -h colab_content_XLM.zip"
      ],
      "metadata": {
        "id": "PCxbAjQ-VzSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uleZh8DfV4-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp colab_content_XLM.zip /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "tMPcluftWIAm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}